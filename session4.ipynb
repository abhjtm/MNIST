{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "session4.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f09Ad9cNoTtQ",
        "colab_type": "code",
        "outputId": "0cd55f46-03b5-48d3-a14d-4259f6283899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization, Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import mnist\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "293IF4toopKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJgCzA1Vo_SK",
        "colab_type": "code",
        "outputId": "a85a150a-51b0-4253-a3b6-e08b426bb786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (10000, 28, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0zB2P_JowZz",
        "colab_type": "code",
        "outputId": "0f38472d-5084-4229-82cd-8379063c63d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "num = np.random.randint(X_train.shape[0])\n",
        "plt.imshow(X_train[num],cmap='gray')\n",
        "print(\"Index: %d, number: %d\"%(num, y_train[num]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index: 27149, number: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwK7iWC8o6gJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0],28,28,1).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0],28,28,1).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXbXI0T4pk73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A5pJH4Ypx3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzJn84-swca4",
        "colab_type": "text"
      },
      "source": [
        "# 1. Plain vanilla model\n",
        "\n",
        "* Number of parameters: 14,322\n",
        "* No batch normalization, no dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rJZnGhsp5bw",
        "colab_type": "code",
        "outputId": "716ad536-7245-43de-d864-8f17d5700dd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(8, 3, activation='relu', input_shape=(28,28,1)))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "\n",
        "model.add(Convolution2D(8,1,activation='relu'))\n",
        "model.add(MaxPooling2D(2))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(Convolution2D(10, 1))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0811 15:16:39.276058 139890028230528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0811 15:16:39.292934 139890028230528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0811 15:16:39.296464 139890028230528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0811 15:16:39.345243 139890028230528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 16)        1168      \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 22, 22, 16)        2320      \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 22, 22, 8)         136       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 9, 16)          1168      \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 5, 5, 16)          2320      \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 3, 3, 16)          2320      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 1, 1, 16)          2320      \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 1, 1, 10)          170       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,322\n",
            "Trainable params: 14,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC1x1A8MqB-j",
        "colab_type": "code",
        "outputId": "e29efece-0794-4a4e-f227-f592715bdcff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0811 15:16:44.021756 139890028230528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0811 15:16:44.030996 139890028230528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcKKRBsosOzK",
        "colab_type": "code",
        "outputId": "88f8e9c8-7624-4de0-cefa-51eed2737890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=1000, epochs=40, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0811 15:16:47.072296 139890028230528 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0811 15:16:47.163091 139890028230528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.9601 - acc: 0.6652 - val_loss: 0.3055 - val_acc: 0.9097\n",
            "Epoch 2/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.2317 - acc: 0.9299 - val_loss: 0.1638 - val_acc: 0.9507\n",
            "Epoch 3/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.1415 - acc: 0.9572 - val_loss: 0.1167 - val_acc: 0.9625\n",
            "Epoch 4/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0989 - acc: 0.9702 - val_loss: 0.0823 - val_acc: 0.9729\n",
            "Epoch 5/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0882 - acc: 0.9739 - val_loss: 0.0736 - val_acc: 0.9758\n",
            "Epoch 6/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0739 - acc: 0.9777 - val_loss: 0.0662 - val_acc: 0.9785\n",
            "Epoch 7/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0655 - acc: 0.9802 - val_loss: 0.0588 - val_acc: 0.9818\n",
            "Epoch 8/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0593 - acc: 0.9816 - val_loss: 0.0695 - val_acc: 0.9775\n",
            "Epoch 9/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0576 - acc: 0.9820 - val_loss: 0.0740 - val_acc: 0.9770\n",
            "Epoch 10/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0532 - acc: 0.9837 - val_loss: 0.0539 - val_acc: 0.9845\n",
            "Epoch 11/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0445 - acc: 0.9860 - val_loss: 0.0479 - val_acc: 0.9860\n",
            "Epoch 12/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0434 - acc: 0.9864 - val_loss: 0.0447 - val_acc: 0.9871\n",
            "Epoch 13/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0385 - acc: 0.9885 - val_loss: 0.0545 - val_acc: 0.9836\n",
            "Epoch 14/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0389 - acc: 0.9878 - val_loss: 0.0408 - val_acc: 0.9882\n",
            "Epoch 15/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0379 - acc: 0.9881 - val_loss: 0.0440 - val_acc: 0.9858\n",
            "Epoch 16/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0345 - acc: 0.9895 - val_loss: 0.0499 - val_acc: 0.9846\n",
            "Epoch 17/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0313 - acc: 0.9896 - val_loss: 0.0429 - val_acc: 0.9876\n",
            "Epoch 18/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0290 - acc: 0.9910 - val_loss: 0.0513 - val_acc: 0.9835\n",
            "Epoch 19/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0318 - acc: 0.9897 - val_loss: 0.0404 - val_acc: 0.9876\n",
            "Epoch 20/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0280 - acc: 0.9911 - val_loss: 0.0462 - val_acc: 0.9865\n",
            "Epoch 21/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0273 - acc: 0.9911 - val_loss: 0.0433 - val_acc: 0.9862\n",
            "Epoch 22/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0271 - acc: 0.9913 - val_loss: 0.0374 - val_acc: 0.9881\n",
            "Epoch 23/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0261 - acc: 0.9915 - val_loss: 0.0417 - val_acc: 0.9870\n",
            "Epoch 24/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0238 - acc: 0.9921 - val_loss: 0.0426 - val_acc: 0.9859\n",
            "Epoch 25/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0223 - acc: 0.9923 - val_loss: 0.0428 - val_acc: 0.9867\n",
            "Epoch 26/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0214 - acc: 0.9931 - val_loss: 0.0411 - val_acc: 0.9874\n",
            "Epoch 27/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0233 - acc: 0.9925 - val_loss: 0.0426 - val_acc: 0.9863\n",
            "Epoch 28/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0217 - acc: 0.9926 - val_loss: 0.0396 - val_acc: 0.9874\n",
            "Epoch 29/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0214 - acc: 0.9927 - val_loss: 0.0367 - val_acc: 0.9891\n",
            "Epoch 30/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0216 - acc: 0.9928 - val_loss: 0.0438 - val_acc: 0.9871\n",
            "Epoch 31/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0175 - acc: 0.9942 - val_loss: 0.0393 - val_acc: 0.9883\n",
            "Epoch 32/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0194 - acc: 0.9938 - val_loss: 0.0392 - val_acc: 0.9879\n",
            "Epoch 33/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0226 - acc: 0.9923 - val_loss: 0.0389 - val_acc: 0.9885\n",
            "Epoch 34/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0208 - acc: 0.9931 - val_loss: 0.0463 - val_acc: 0.9872\n",
            "Epoch 35/40\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0195 - acc: 0.9934 - val_loss: 0.0413 - val_acc: 0.9881\n",
            "Epoch 36/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0153 - acc: 0.9948 - val_loss: 0.0443 - val_acc: 0.9869\n",
            "Epoch 37/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0170 - acc: 0.9945 - val_loss: 0.0397 - val_acc: 0.9887\n",
            "Epoch 38/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0168 - acc: 0.9947 - val_loss: 0.0369 - val_acc: 0.9890\n",
            "Epoch 39/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0141 - acc: 0.9953 - val_loss: 0.0396 - val_acc: 0.9891\n",
            "Epoch 40/40\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0148 - acc: 0.9946 - val_loss: 0.0471 - val_acc: 0.9877\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3a64d09978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qZgl1uasgZM",
        "colab_type": "text"
      },
      "source": [
        "# Results:\n",
        "## After 40 epochs:\n",
        "* validation accuracy peaked to 98.9%\n",
        "* training accuracy hovered around 99.4%\n",
        "\n",
        "## After 80 epochs:\n",
        "* validation accuracy remained at ~99%\n",
        "* training accuracy peaked to 99.7%\n",
        "\n",
        "This is a clear case of overfitting. This can be avoided by using dropout.\n",
        "\n",
        "---------\n",
        "\n",
        "# 2. Model with BatchNormalization and Dropout\n",
        "This is done to avoid the case of overfitting as seen above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhBC4EEgytVH",
        "colab_type": "code",
        "outputId": "819c5260-19aa-454d-e156-e9eeeff853d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(8, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(8,1,activation='relu'))\n",
        "model.add(MaxPooling2D(2))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, 1))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0811 15:18:32.193546 139890028230528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0811 15:18:32.261250 139890028230528 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_11 (Conv2D)           (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 24, 24, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 22, 22, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 22, 22, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 22, 22, 8)         136       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 8)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 11, 11, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 9, 9, 16)          1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 5, 5, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 5, 5, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 3, 3, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 3, 3, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 3, 3, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 1, 1, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 1, 1, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 1, 1, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 1, 1, 10)          170       \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,802\n",
            "Trainable params: 14,562\n",
            "Non-trainable params: 240\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogsYWm7U0zR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hky4bz1e03g2",
        "colab_type": "code",
        "outputId": "c76b3d6f-cbb3-498d-9b23-111531a0c665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=1000, epochs=40, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 1.0717 - acc: 0.6879 - val_loss: 0.3489 - val_acc: 0.8850\n",
            "Epoch 2/40\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2815 - acc: 0.9305 - val_loss: 0.1109 - val_acc: 0.9670\n",
            "Epoch 3/40\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.1504 - acc: 0.9612 - val_loss: 0.0674 - val_acc: 0.9786\n",
            "Epoch 4/40\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.1070 - acc: 0.9708 - val_loss: 0.0581 - val_acc: 0.9806\n",
            "Epoch 5/40\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0813 - acc: 0.9775 - val_loss: 0.0401 - val_acc: 0.9871\n",
            "Epoch 6/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0708 - acc: 0.9803 - val_loss: 0.0352 - val_acc: 0.9885\n",
            "Epoch 7/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0637 - acc: 0.9816 - val_loss: 0.0329 - val_acc: 0.9896\n",
            "Epoch 8/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0566 - acc: 0.9832 - val_loss: 0.0328 - val_acc: 0.9892\n",
            "Epoch 9/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0531 - acc: 0.9845 - val_loss: 0.0279 - val_acc: 0.9906\n",
            "Epoch 10/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0491 - acc: 0.9861 - val_loss: 0.0268 - val_acc: 0.9914\n",
            "Epoch 11/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0481 - acc: 0.9857 - val_loss: 0.0262 - val_acc: 0.9922\n",
            "Epoch 12/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0458 - acc: 0.9862 - val_loss: 0.0294 - val_acc: 0.9909\n",
            "Epoch 13/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0433 - acc: 0.9871 - val_loss: 0.0273 - val_acc: 0.9924\n",
            "Epoch 14/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0413 - acc: 0.9878 - val_loss: 0.0230 - val_acc: 0.9936\n",
            "Epoch 15/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0405 - acc: 0.9881 - val_loss: 0.0291 - val_acc: 0.9912\n",
            "Epoch 16/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0399 - acc: 0.9879 - val_loss: 0.0278 - val_acc: 0.9917\n",
            "Epoch 17/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0383 - acc: 0.9879 - val_loss: 0.0249 - val_acc: 0.9927\n",
            "Epoch 18/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0383 - acc: 0.9882 - val_loss: 0.0284 - val_acc: 0.9912\n",
            "Epoch 19/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0352 - acc: 0.9892 - val_loss: 0.0304 - val_acc: 0.9902\n",
            "Epoch 20/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0357 - acc: 0.9894 - val_loss: 0.0234 - val_acc: 0.9932\n",
            "Epoch 21/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0338 - acc: 0.9896 - val_loss: 0.0241 - val_acc: 0.9924\n",
            "Epoch 22/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0325 - acc: 0.9901 - val_loss: 0.0223 - val_acc: 0.9931\n",
            "Epoch 23/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0342 - acc: 0.9895 - val_loss: 0.0232 - val_acc: 0.9931\n",
            "Epoch 24/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0337 - acc: 0.9898 - val_loss: 0.0241 - val_acc: 0.9923\n",
            "Epoch 25/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0307 - acc: 0.9908 - val_loss: 0.0209 - val_acc: 0.9940\n",
            "Epoch 26/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0309 - acc: 0.9902 - val_loss: 0.0236 - val_acc: 0.9932\n",
            "Epoch 27/40\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0285 - acc: 0.9907 - val_loss: 0.0205 - val_acc: 0.9938\n",
            "Epoch 28/40\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0279 - acc: 0.9910 - val_loss: 0.0235 - val_acc: 0.9931\n",
            "Epoch 29/40\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0297 - acc: 0.9909 - val_loss: 0.0272 - val_acc: 0.9933\n",
            "Epoch 30/40\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0297 - acc: 0.9908 - val_loss: 0.0220 - val_acc: 0.9935\n",
            "Epoch 31/40\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.0278 - acc: 0.9916 - val_loss: 0.0217 - val_acc: 0.9932\n",
            "Epoch 32/40\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.0280 - acc: 0.9915 - val_loss: 0.0204 - val_acc: 0.9938\n",
            "Epoch 33/40\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0287 - acc: 0.9911 - val_loss: 0.0244 - val_acc: 0.9925\n",
            "Epoch 34/40\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0279 - acc: 0.9911 - val_loss: 0.0218 - val_acc: 0.9930\n",
            "Epoch 35/40\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.0271 - acc: 0.9917 - val_loss: 0.0218 - val_acc: 0.9931\n",
            "Epoch 36/40\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0261 - acc: 0.9917 - val_loss: 0.0239 - val_acc: 0.9930\n",
            "Epoch 37/40\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0267 - acc: 0.9921 - val_loss: 0.0213 - val_acc: 0.9932\n",
            "Epoch 38/40\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0249 - acc: 0.9922 - val_loss: 0.0227 - val_acc: 0.9934\n",
            "Epoch 39/40\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.0245 - acc: 0.9925 - val_loss: 0.0221 - val_acc: 0.9930\n",
            "Epoch 40/40\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0241 - acc: 0.9925 - val_loss: 0.0279 - val_acc: 0.9909\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3a07800be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9LXok_u055x",
        "colab_type": "text"
      },
      "source": [
        "# Results\n",
        "## After 40 epochs:\n",
        "* Training accuracy peaks to 99.25%\n",
        "* Validation accuracy touched 99.4%\n",
        "\n",
        "Note: We have already hit the target of 99.4% validation accuracy.\n",
        "\n",
        "Training accuracy isn't increasing much and validation accuracy is exceeding it. This suggests that there might be underfitting. Trying to reduce the number of dropout layers\n",
        "\n",
        "----------\n",
        "# 3. Model with lesser dropouts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugqlALbCEhvz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2fd1f322-5660-4b07-8c5e-2e1ac8330f6f"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(8, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(8,1,activation='relu'))\n",
        "model.add(MaxPooling2D(2))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(10, 1))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_51 (Conv2D)           (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 24, 24, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 22, 22, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 22, 22, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_54 (Conv2D)           (None, 22, 22, 8)         136       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 11, 11, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_55 (Conv2D)           (None, 9, 9, 16)          1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_56 (Conv2D)           (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_57 (Conv2D)           (None, 5, 5, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_38 (Batc (None, 5, 5, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_58 (Conv2D)           (None, 3, 3, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 3, 3, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 3, 3, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_59 (Conv2D)           (None, 1, 1, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 1, 1, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_60 (Conv2D)           (None, 1, 1, 10)          170       \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,802\n",
            "Trainable params: 14,562\n",
            "Non-trainable params: 240\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HiycPPC_vfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1ajrlg0_zkX",
        "colab_type": "code",
        "outputId": "4467e023-02d1-4ebf-dd38-0b17cb365499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=10000, epochs=40, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.0320 - acc: 0.9902 - val_loss: 0.0269 - val_acc: 0.9928\n",
            "Epoch 2/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0302 - acc: 0.9909 - val_loss: 0.0269 - val_acc: 0.9920\n",
            "Epoch 3/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0264 - acc: 0.9918 - val_loss: 0.0258 - val_acc: 0.9928\n",
            "Epoch 4/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0268 - acc: 0.9919 - val_loss: 0.0244 - val_acc: 0.9928\n",
            "Epoch 5/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0256 - acc: 0.9922 - val_loss: 0.0236 - val_acc: 0.9931\n",
            "Epoch 6/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0262 - acc: 0.9920 - val_loss: 0.0231 - val_acc: 0.9932\n",
            "Epoch 7/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0254 - acc: 0.9921 - val_loss: 0.0228 - val_acc: 0.9935\n",
            "Epoch 8/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0245 - acc: 0.9925 - val_loss: 0.0226 - val_acc: 0.9932\n",
            "Epoch 9/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0251 - acc: 0.9921 - val_loss: 0.0230 - val_acc: 0.9932\n",
            "Epoch 10/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0237 - acc: 0.9928 - val_loss: 0.0236 - val_acc: 0.9932\n",
            "Epoch 11/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0242 - acc: 0.9927 - val_loss: 0.0235 - val_acc: 0.9934\n",
            "Epoch 12/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0238 - acc: 0.9930 - val_loss: 0.0229 - val_acc: 0.9939\n",
            "Epoch 13/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0235 - acc: 0.9927 - val_loss: 0.0229 - val_acc: 0.9936\n",
            "Epoch 14/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0234 - acc: 0.9929 - val_loss: 0.0229 - val_acc: 0.9935\n",
            "Epoch 15/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0235 - acc: 0.9930 - val_loss: 0.0219 - val_acc: 0.9939\n",
            "Epoch 16/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0228 - acc: 0.9931 - val_loss: 0.0220 - val_acc: 0.9935\n",
            "Epoch 17/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0224 - acc: 0.9933 - val_loss: 0.0230 - val_acc: 0.9930\n",
            "Epoch 18/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0223 - acc: 0.9935 - val_loss: 0.0234 - val_acc: 0.9930\n",
            "Epoch 19/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0228 - acc: 0.9930 - val_loss: 0.0235 - val_acc: 0.9937\n",
            "Epoch 20/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0215 - acc: 0.9932 - val_loss: 0.0233 - val_acc: 0.9936\n",
            "Epoch 21/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0217 - acc: 0.9934 - val_loss: 0.0230 - val_acc: 0.9935\n",
            "Epoch 22/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0223 - acc: 0.9934 - val_loss: 0.0233 - val_acc: 0.9933\n",
            "Epoch 23/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0213 - acc: 0.9940 - val_loss: 0.0230 - val_acc: 0.9937\n",
            "Epoch 24/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0214 - acc: 0.9935 - val_loss: 0.0236 - val_acc: 0.9936\n",
            "Epoch 25/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0213 - acc: 0.9934 - val_loss: 0.0234 - val_acc: 0.9937\n",
            "Epoch 26/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0216 - acc: 0.9935 - val_loss: 0.0225 - val_acc: 0.9934\n",
            "Epoch 27/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0215 - acc: 0.9935 - val_loss: 0.0222 - val_acc: 0.9939\n",
            "Epoch 28/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0206 - acc: 0.9934 - val_loss: 0.0226 - val_acc: 0.9936\n",
            "Epoch 29/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0202 - acc: 0.9937 - val_loss: 0.0224 - val_acc: 0.9936\n",
            "Epoch 30/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0209 - acc: 0.9935 - val_loss: 0.0229 - val_acc: 0.9937\n",
            "Epoch 31/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0206 - acc: 0.9937 - val_loss: 0.0226 - val_acc: 0.9937\n",
            "Epoch 32/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0203 - acc: 0.9936 - val_loss: 0.0227 - val_acc: 0.9942\n",
            "Epoch 33/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0197 - acc: 0.9940 - val_loss: 0.0228 - val_acc: 0.9939\n",
            "Epoch 34/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0196 - acc: 0.9944 - val_loss: 0.0223 - val_acc: 0.9937\n",
            "Epoch 35/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0200 - acc: 0.9941 - val_loss: 0.0226 - val_acc: 0.9937\n",
            "Epoch 36/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0196 - acc: 0.9938 - val_loss: 0.0235 - val_acc: 0.9933\n",
            "Epoch 37/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0198 - acc: 0.9940 - val_loss: 0.0234 - val_acc: 0.9933\n",
            "Epoch 38/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0188 - acc: 0.9940 - val_loss: 0.0229 - val_acc: 0.9940\n",
            "Epoch 39/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0202 - acc: 0.9936 - val_loss: 0.0231 - val_acc: 0.9935\n",
            "Epoch 40/40\n",
            "60000/60000 [==============================] - 3s 45us/step - loss: 0.0191 - acc: 0.9941 - val_loss: 0.0237 - val_acc: 0.9934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f39ad0f4d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjyYa9hk3xIz",
        "colab_type": "text"
      },
      "source": [
        "# Results\n",
        "* Both training and validation accuracies hover between 99.3-99.4% suggesting appropriate regularization. So no need to add more dropout or remove existing dropout layers.\n",
        "* It is possible that training accuracies are not improving further because of a higher learning rate. This higher learning rate is inhibiting the process of finding the minima of the loss function.\n",
        "\n",
        "# 4. Varying LR\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J4nOvNEUIdD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "66050626-df25-4327-ac3d-55e60029af76"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(8, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(8,1,activation='relu'))\n",
        "model.add(MaxPooling2D(2))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(16, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(10, 1))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_91 (Conv2D)           (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_65 (Batc (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "conv2d_92 (Conv2D)           (None, 24, 24, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_66 (Batc (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_57 (Dropout)         (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_93 (Conv2D)           (None, 22, 22, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_67 (Batc (None, 22, 22, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_58 (Dropout)         (None, 22, 22, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_94 (Conv2D)           (None, 22, 22, 8)         136       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 11, 11, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_95 (Conv2D)           (None, 9, 9, 16)          1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_68 (Batc (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_59 (Dropout)         (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_96 (Conv2D)           (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_69 (Batc (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_60 (Dropout)         (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_97 (Conv2D)           (None, 5, 5, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_70 (Batc (None, 5, 5, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_61 (Dropout)         (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_98 (Conv2D)           (None, 3, 3, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_71 (Batc (None, 3, 3, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_62 (Dropout)         (None, 3, 3, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_99 (Conv2D)           (None, 1, 1, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_72 (Batc (None, 1, 1, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_100 (Conv2D)          (None, 1, 1, 10)          170       \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,802\n",
            "Trainable params: 14,562\n",
            "Non-trainable params: 240\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Euufwf80Uyot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  return round(0.01 * 1/(1 + 0.3 * epoch), 10)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP4q10xPUN5a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fec7eb31-6fcf-4775-f7fa-d456d7e0bec8"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=1000, epochs=40, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.01.\n",
            "60000/60000 [==============================] - 10s 168us/step - loss: 0.5033 - acc: 0.8599 - val_loss: 0.1506 - val_acc: 0.9539\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0076923077.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0836 - acc: 0.9756 - val_loss: 0.0623 - val_acc: 0.9793\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.00625.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0615 - acc: 0.9817 - val_loss: 0.0351 - val_acc: 0.9891\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0052631579.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0521 - acc: 0.9844 - val_loss: 0.0365 - val_acc: 0.9881\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0045454545.\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 0.0440 - acc: 0.9866 - val_loss: 0.0326 - val_acc: 0.9895\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.004.\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 0.0405 - acc: 0.9879 - val_loss: 0.0399 - val_acc: 0.9876\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0035714286.\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 0.0364 - acc: 0.9888 - val_loss: 0.0297 - val_acc: 0.9901\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0032258065.\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 0.0337 - acc: 0.9900 - val_loss: 0.0320 - val_acc: 0.9898\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0029411765.\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 0.0317 - acc: 0.9902 - val_loss: 0.0234 - val_acc: 0.9920\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0027027027.\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 0.0292 - acc: 0.9909 - val_loss: 0.0286 - val_acc: 0.9916\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0025.\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 0.0280 - acc: 0.9912 - val_loss: 0.0290 - val_acc: 0.9913\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0023255814.\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 0.0277 - acc: 0.9914 - val_loss: 0.0354 - val_acc: 0.9893\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.002173913.\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 0.0256 - acc: 0.9920 - val_loss: 0.0253 - val_acc: 0.9928\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0020408163.\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 0.0246 - acc: 0.9928 - val_loss: 0.0243 - val_acc: 0.9919\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0019230769.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0237 - acc: 0.9924 - val_loss: 0.0234 - val_acc: 0.9933\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0018181818.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0236 - acc: 0.9922 - val_loss: 0.0258 - val_acc: 0.9922\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0017241379.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0226 - acc: 0.9930 - val_loss: 0.0227 - val_acc: 0.9933\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0016393443.\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 0.0223 - acc: 0.9928 - val_loss: 0.0263 - val_acc: 0.9913\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0015625.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0227 - acc: 0.9931 - val_loss: 0.0255 - val_acc: 0.9921\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0014925373.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0210 - acc: 0.9933 - val_loss: 0.0220 - val_acc: 0.9932\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0014285714.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0204 - acc: 0.9936 - val_loss: 0.0233 - val_acc: 0.9934\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.001369863.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0192 - acc: 0.9936 - val_loss: 0.0214 - val_acc: 0.9938\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0013157895.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0196 - acc: 0.9938 - val_loss: 0.0250 - val_acc: 0.9921\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0012658228.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0192 - acc: 0.9941 - val_loss: 0.0219 - val_acc: 0.9938\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0012195122.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0191 - acc: 0.9940 - val_loss: 0.0207 - val_acc: 0.9926\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0011764706.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0177 - acc: 0.9946 - val_loss: 0.0216 - val_acc: 0.9935\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0011363636.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0179 - acc: 0.9941 - val_loss: 0.0216 - val_acc: 0.9926\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0010989011.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0181 - acc: 0.9940 - val_loss: 0.0208 - val_acc: 0.9931\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0010638298.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0178 - acc: 0.9944 - val_loss: 0.0202 - val_acc: 0.9934\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0010309278.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0155 - acc: 0.9952 - val_loss: 0.0217 - val_acc: 0.9932\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.001.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0162 - acc: 0.9949 - val_loss: 0.0201 - val_acc: 0.9935\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0009708738.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0155 - acc: 0.9952 - val_loss: 0.0180 - val_acc: 0.9943\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0009433962.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0155 - acc: 0.9950 - val_loss: 0.0239 - val_acc: 0.9926\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0009174312.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0162 - acc: 0.9949 - val_loss: 0.0214 - val_acc: 0.9932\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0008928571.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0158 - acc: 0.9951 - val_loss: 0.0212 - val_acc: 0.9928\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0008695652.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0150 - acc: 0.9953 - val_loss: 0.0226 - val_acc: 0.9933\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0008474576.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0149 - acc: 0.9952 - val_loss: 0.0212 - val_acc: 0.9936\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0008264463.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0151 - acc: 0.9950 - val_loss: 0.0200 - val_acc: 0.9933\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0008064516.\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0136 - acc: 0.9955 - val_loss: 0.0187 - val_acc: 0.9942\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0007874016.\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 0.0141 - acc: 0.9952 - val_loss: 0.0205 - val_acc: 0.9942\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f37a937a278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWwNjjdzUYvU",
        "colab_type": "text"
      },
      "source": [
        "# Results\n",
        "## Validation accuracy: 99.42%\n",
        "Trained for 40 epochs and achieved a validation accuracy of 99.42%.  Total 14,802 parameters used."
      ]
    }
  ]
}